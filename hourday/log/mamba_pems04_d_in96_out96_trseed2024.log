nohup: ignoring input
Args in experiment:
Namespace(activation='gelu', anomaly_ratio=0.25, batch_size=32, c_out=822, checkpoints='./checkpoints/', d_conv=4, d_ff=16, d_layers=1, d_model=128, data='custom', data_path='pems04_d.csv', dec_in=151, des='Exp', devices='0,1,2,3', distil=True, dropout=0, e_layers=2, embed='timeF', enc_in=822, expand=2, factor=1, features='M', freq='h', gap_day=365, gpu=0, inverse=False, is_training=1, itr=1, label_len=48, learning_rate=0.0001, loss='MSE', lradj='type1', mask_rate=0.25, model='Mamba', model_id='pems04_d_96_96', moving_avg=25, n_heads=8, num_kernels=6, num_workers=10, output_attention=False, p_hidden_dims=[128, 128], p_hidden_layers=2, patience=3, pred_len=96, root_path='../../data/pems/', samle_rate=1.0, sample_seed=7, seasonal_patterns='Monthly', seq_len=96, target='OT', task_name='long_term_forecast', top_k=5, train_epochs=10, train_seed=2024, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_pems04_d_96_96_Mamba_custom_ftM_sl96_ll48_pl96_dm128_nh8_el2_dl1_df16_fc1_ebtimeF_dtTrue_srate1.0_sseed7_trainseed2024_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 5383
val 703
test 1497
	iters: 100, epoch: 1 | loss: 0.4523701
	speed: 0.0503s/iter; left time: 79.5208s
Epoch: 1 cost time: 6.872009038925171
Epoch: 1, Steps: 168 | Train Loss: 0.5969171 Vali Loss: 0.4240664 Test Loss: 0.6720355
Validation loss decreased (inf --> 0.424066).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.4661610
	speed: 0.1335s/iter; left time: 188.5953s
Epoch: 2 cost time: 5.7075090408325195
Epoch: 2, Steps: 168 | Train Loss: 0.4768797 Vali Loss: 0.4312383 Test Loss: 0.6487488
EarlyStopping counter: 1 out of 3
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.4484500
	speed: 0.1369s/iter; left time: 170.4106s
Epoch: 3 cost time: 5.735926866531372
Epoch: 3, Steps: 168 | Train Loss: 0.4424784 Vali Loss: 0.4447626 Test Loss: 0.6501825
EarlyStopping counter: 2 out of 3
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.4421046
	speed: 0.1388s/iter; left time: 149.4782s
Epoch: 4 cost time: 5.6955883502960205
Epoch: 4, Steps: 168 | Train Loss: 0.4304318 Vali Loss: 0.4465016 Test Loss: 0.6520395
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : long_term_forecast_pems04_d_96_96_Mamba_custom_ftM_sl96_ll48_pl96_dm128_nh8_el2_dl1_df16_fc1_ebtimeF_dtTrue_srate1.0_sseed7_trainseed2024_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 1497
test shape: (1497, 1, 96, 822) (1497, 1, 96, 822)
test shape: (1497, 96, 822) (1497, 96, 822)
mse:0.6720369458198547, mae:0.49872657656669617
>>>>>>>Overall time: 68 seconds<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
nohup: ignoring input
Args in experiment:
Namespace(activation='gelu', anomaly_ratio=0.25, batch_size=32, c_out=822, checkpoints='./checkpoints/', d_conv=4, d_ff=16, d_layers=1, d_model=128, data='custom', data_path='pems04_d.csv', dec_in=151, des='Exp', devices='0,1,2,3', distil=True, dropout=0, e_layers=2, embed='timeF', enc_in=822, expand=2, factor=1, features='M', freq='h', gap_day=365, gpu=0, inverse=False, is_training=1, itr=1, label_len=48, learning_rate=0.0001, loss='MSE', lradj='type1', mask_rate=0.25, model='Mamba', model_id='pems04_d_96_336', moving_avg=25, n_heads=8, num_kernels=6, num_workers=10, output_attention=False, p_hidden_dims=[128, 128], p_hidden_layers=2, patience=3, pred_len=336, root_path='../../data/pems/', samle_rate=1.0, sample_seed=7, seasonal_patterns='Monthly', seq_len=336, target='OT', task_name='long_term_forecast', top_k=5, train_epochs=10, train_seed=2024, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_pems04_d_96_336_Mamba_custom_ftM_sl336_ll48_pl336_dm128_nh8_el2_dl1_df16_fc1_ebtimeF_dtTrue_srate1.0_sseed7_trainseed2024_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 4903
val 463
test 1257
	iters: 100, epoch: 1 | loss: 0.7018837
	speed: 0.1425s/iter; left time: 203.9610s
Epoch: 1 cost time: 20.012502193450928
Epoch: 1, Steps: 153 | Train Loss: 0.7286719 Vali Loss: 0.5201622 Test Loss: 0.7945642
Validation loss decreased (inf --> 0.520162).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.5274329
	speed: 0.2951s/iter; left time: 377.0871s
Epoch: 2 cost time: 13.23464560508728
Epoch: 2, Steps: 153 | Train Loss: 0.5677777 Vali Loss: 0.5365780 Test Loss: 0.8025815
EarlyStopping counter: 1 out of 3
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.4913620
	speed: 0.2539s/iter; left time: 285.6240s
Epoch: 3 cost time: 12.000221014022827
Epoch: 3, Steps: 153 | Train Loss: 0.4804178 Vali Loss: 0.5387974 Test Loss: 0.7972212
EarlyStopping counter: 2 out of 3
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.4109874
	speed: 0.2439s/iter; left time: 237.0885s
